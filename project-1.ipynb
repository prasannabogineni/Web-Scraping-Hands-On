{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98462e83",
   "metadata": {},
   "source": [
    "In this kaggle kernel, I'm going to do web scraping of Website \"https://www.holidify.com/explore/\" Page. This Webpage Contain 60 Places to Visit in India and I'm going to scrap Those 60 Places information like Name of the Place, then the State it is in, then Average Rating given by people who went there, then short Paragraph describing the Place, then the Total Estimated Price, then finally there numbers of major attraction points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a4f96",
   "metadata": {},
   "source": [
    "First we Import all the Important Packages that are required to do Web Scraping.\n",
    "\n",
    "Numpy and Pandas are standard and I always import it, who knows when it come handy.\n",
    "\n",
    "re is the package required to do Regular Expression, with the help of this package, we can easily search for particular type of words or digits in a string, and re is one of the most important string manipulation package.\n",
    "\n",
    "requests is a Python module that you can use to send all kinds of HTTP requests. It is an easy-to-use library with a lot of features ranging from passing parameters in URLs to sending custom headers and SSL Verification.\n",
    "\n",
    "BeautifulSoup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It saves hours or days of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6f8414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4a618",
   "metadata": {},
   "source": [
    "And here comes the link, first we copy and paste the url and assigned it to the variable link, then we send a request to the web page to reture the information.\n",
    "\n",
    "After requesting, we will print out the value like I did here print(P_link) , which gives the output of \"<Response [200]>\" or \"200\" which means you are allowed to do web scraping on such websites.\n",
    "\n",
    "After Getting the Response we will convert the HTML page into readable form of text and assigned it to the variable P_html.\n",
    "\n",
    "Then we use Beatiful Soup to Convert the HTML page in to readable form by passing \"html.parser\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107a449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.holidify.com/explore/\"\n",
    "P_link = requests.get(link)\n",
    "print(P_link)\n",
    "\n",
    "P_html = P_link.text\n",
    "P_soup = BeautifulSoup(P_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a5145",
   "metadata": {},
   "source": [
    "Here we first go to the First Table of the Website Holidify and open the HTML page by Right Clicking, then go to Inspect Element.\n",
    "\n",
    "Where we find that the all the values of the first table is in the \"div\" \"Class\" of \"col-12 col-md-6 pr-md-3 result\".\n",
    "\n",
    "Thus we type findAll value in our P_soup inside the div class of \"col-12 col-md-6 pr-md-3 result\".\n",
    "\n",
    "At last we will print the len of the total numbers of table on one webpage. Here in this case there are 60 values on first WebPage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdc2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "containers = P_soup.findAll(\"div\", {\"class\" : \"col-12 col-md-6 pr-md-3\"})\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc9ea5",
   "metadata": {},
   "source": [
    "Then here we assigned the first value in the table with containers[0] and assigned it a new variable container. We will do all the scraping work on the first value of table first then we move on create a loop and do it for the rest of table.\n",
    "\n",
    "After that we create a new DataFrame which became our actual file, which contain all the data, which we want to create from the start.\n",
    "\n",
    "We create a list of values and assigned a name column,this will become our columns name of our Data Places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d8b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = containers[0]\n",
    "column = ['Place','State', 'Ratings', 'About','Price', 'Attraction']\n",
    "Places = pd.DataFrame(columns = column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d79fdd",
   "metadata": {},
   "source": [
    "Here we create a loop and print all the Values which we want to extract from the Webpage, here I want to import values like Places Name, City Name, Ratings, About, Prices, Attraction.\n",
    "\n",
    "Remember it's not always necessery that each value you want to extract are always available separately or will be in the structured form, we have to string manipulation to clean the data and also split() the values to create two or more values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29da4d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m container \u001b[38;5;129;01min\u001b[39;00m containers:\n\u001b[0;32m      2\u001b[0m     p_nameP \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcard-heading\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m----> 3\u001b[0m     p_nameN \u001b[38;5;241m=\u001b[39m \u001b[43mp_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(p_nameN) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:        \n\u001b[0;32m      5\u001b[0m           p_nameP \u001b[38;5;241m=\u001b[39m p_nameN[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for container in containers:\n",
    "    p_name = container.findAll(\"h2\", {\"class\":\"card-heading\"})\n",
    "    p_nameN = p_name[0].text[4:].strip().split()\n",
    "    if len(p_nameN) == 2:        \n",
    "          p_nameP = p_nameN[0]\n",
    "          p_nameP = p_nameP.replace(',','')\n",
    "          p_nameC = p_nameN[1]\n",
    "    elif len(p_nameN) == 3:\n",
    "          p_nameP = p_nameN[0]\n",
    "          p_nameP = p_nameP.replace(',','')\n",
    "          p_nameC = p_nameN[1] + \" \" + p_nameN[2]\n",
    "    elif len(p_nameN) == 4:\n",
    "          p_nameP = p_nameN[0]\n",
    "          p_nameP = p_nameP.replace(',','')\n",
    "          p_nameC = p_nameN[1] + \" \" + p_nameN[2] + \" \" + p_nameN[3]      \n",
    "    else:\n",
    "          p_nameP = p_nameN[0]\n",
    "          p_nameC = \"NaN\"\n",
    "    p_rating = container.findAll(\"span\", {\"class\" : \"rating-badge\"})\n",
    "    p_rating = p_rating[0].text[1:4]\n",
    "    p_about = container.findAll(\"p\",{\"class\": \"card-text\"})\n",
    "    p_about = p_about[0].text\n",
    "    p_price = container.findAll(\"p\",{\"class\": \"collection-cta\"})\n",
    "    if len(p_price) == 1:\n",
    "        p_num = p_price[0].text.replace(',','')\n",
    "        p_numb = re.findall(r'\\d+', p_num)\n",
    "        num = \"\"\n",
    "        for i in p_numb:\n",
    "            num += i\n",
    "    else:\n",
    "        num = \"NaN\"\n",
    "    p_attraction = container.findAll(\"div\", {\"class\":\"content-card-footer\"})\n",
    "    p_attraction = p_attraction[0].text[:-12].strip()\n",
    "    \n",
    "    Data = pd.DataFrame([[p_nameP ,p_nameC, p_rating , p_about, num , p_attraction]])\n",
    "    Data.columns = column\n",
    "    Places = Places.append(Data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91defdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Place, State, Ratings, About, Price, Attraction]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(Places.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c0e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
